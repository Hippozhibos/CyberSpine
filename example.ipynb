{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 11 09:28:27 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:4B:00.0 Off |                  Off |\n",
      "|  0%   31C    P8             20W /  450W |     442MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off |   00000000:B1:00.0 Off |                  Off |\n",
      "|  0%   31C    P8             26W /  450W |      44MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      4471      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    0   N/A  N/A     29175      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    0   N/A  N/A     36572      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    0   N/A  N/A   1644816      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    0   N/A  N/A   1648660      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    0   N/A  N/A   1649031    C+G   ...libexec/gnome-remote-desktop-daemon        390MiB |\n",
      "|    1   N/A  N/A      4471      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A     29175      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A     36572      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A   1644816      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A   1648660      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "env: MUJOCO_GL=egl\n",
      "Checking that the dm_control installation succeeded...\n",
      "Installed dm_control 1.0.22\n"
     ]
    }
   ],
   "source": [
    "#@title Run to install MuJoCo and `dm_control`\n",
    "import distutils.util\n",
    "import os\n",
    "import subprocess\n",
    "if subprocess.run('nvidia-smi').returncode:\n",
    "  raise RuntimeError(\n",
    "      'Cannot communicate with GPU. '\n",
    "      'Make sure you are using a GPU Colab runtime. '\n",
    "      'Go to the Runtime menu and select Choose runtime type.')\n",
    "\n",
    "# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
    "# This is usually installed as part of an Nvidia driver package, but the Colab\n",
    "# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
    "# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
    "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
    "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
    "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
    "    f.write(\"\"\"{\n",
    "    \"file_format_version\" : \"1.0.0\",\n",
    "    \"ICD\" : {\n",
    "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# print('Installing dm_control...')\n",
    "# !pip install -q dm_control>=1.0.18\n",
    "\n",
    "# Configure dm_control to use the EGL rendering backend (requires GPU)\n",
    "%env MUJOCO_GL=egl\n",
    "\n",
    "print('Checking that the dm_control installation succeeded...')\n",
    "try:\n",
    "  from dm_control import suite\n",
    "  env = suite.load('cartpole', 'swingup')\n",
    "  pixels = env.physics.render()\n",
    "except Exception as e:\n",
    "  raise e from RuntimeError(\n",
    "      'Something went wrong during installation. Check the shell output above '\n",
    "      'for more information.\\n'\n",
    "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
    "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
    "else:\n",
    "  del pixels, suite\n",
    "\n",
    "!echo Installed dm_control $(pip show dm_control | grep -Po \"(?<=Version: ).+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Other imports and helper functions\n",
    "\n",
    "# General\n",
    "import copy\n",
    "import os\n",
    "import itertools\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "# Graphics-related\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import PIL.Image\n",
    "# Internal loading of video libraries.\n",
    "\n",
    "# Use svg backend for figure rendering\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# Font sizes\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 10\n",
    "BIGGER_SIZE = 12\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "# Inline video helper function\n",
    "if os.environ.get('COLAB_NOTEBOOK_TEST', False):\n",
    "  # We skip video generation during tests, as it is quite expensive.\n",
    "  display_video = lambda *args, **kwargs: None\n",
    "else:\n",
    "  def display_video(frames, framerate=30):\n",
    "    height, width, _ = frames[0].shape\n",
    "    dpi = 70\n",
    "\n",
    "    orig_backend = matplotlib.get_backend()\n",
    "    matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "    # fig, ax = plt.subplots(1, 1, figsize=(scaled_width / dpi, scaled_height / dpi), dpi=dpi)\n",
    "    matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
    "    ax.set_axis_off()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_position([0, 0, 1, 1])\n",
    "    im = ax.imshow(frames[0])\n",
    "    def update(frame):\n",
    "      im.set_data(frame)\n",
    "      return [im]\n",
    "    interval = 1000/framerate\n",
    "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                   interval=interval, blit=True, repeat=False)\n",
    "    return HTML(anim.to_html5_video())\n",
    "\n",
    "# Seed numpy's global RNG so that cell outputs are deterministic. We also try to\n",
    "# use RandomState instances that are local to a single cell wherever possible.\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangzhibo/anaconda3/envs/mujoco/lib/python3.11/site-packages/glfw/__init__.py:914: GLFWError: (65544) b'X11: The DISPLAY environment variable is missing'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dm_control import viewer\n",
    "from env import mice_env\n",
    "# from spine import prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('Humanoid-v4', ctrl_cost_weight=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = mice_env.rodent_maze_forage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.physics.proprioception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_data = env.physics.data.sensordata\n",
    "sensor = env.physics.data.sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for sensor_id in range(env.physics.model.nsensor):\n",
    "    sensor_name = env.physics.model.id2name(sensor_id, 'sensor')\n",
    "    sensor_dim = env.physics.model.sensor_dim[sensor_id]  # 获取该传感器的维度\n",
    "    sensor_values = env.physics.data.sensordata[index:index + sensor_dim]\n",
    "    print(f\"Sensor {sensor_id}: {sensor_name} - Values: {sensor_values}\")\n",
    "    index += sensor_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_spec = env.action_spec()\n",
    "time_step = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step.observation['walker/world_zaxis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scene = np.dot(scene[..., :3], [0.2989, 0.5870, 0.1140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 绘制图像\n",
    "plt.imshow(scene)\n",
    "plt.axis('off')  # 隐藏坐标轴\n",
    "plt.title('Egocentric Camera View')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_size = {'width': 640, 'height': 480}\n",
    "pixels = env.physics.render(camera_id=1,**frame_size)\n",
    "PIL.Image.fromarray(pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(time_step):\n",
    "    # Generates random actions within the specified range\n",
    "    action = 0.01 * np.random.uniform(action_spec.minimum, action_spec.maximum,\n",
    "                               size=action_spec.shape)\n",
    "    # print(\"reward = {}, discount = {}, observations = {}.\".format(\n",
    "    #   time_step.reward, time_step.discount, time_step.observation))\n",
    "    return np.clip(action, action_spec.minimum, action_spec.maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PD_policy(time_step):\n",
    "    # Access the current positions and velocities\n",
    "    qpos = env.physics.data.qpos[7:]\n",
    "    qvel = env.physics.data.qvel[6:]\n",
    "\n",
    "    # Define desired positions and velocities (for simplicity, assume you want to maintain the current position)\n",
    "    desired_qpos = np.zeros_like(qpos)\n",
    "    desired_qvel = np.zeros_like(qvel)\n",
    "\n",
    "    # Proportional-Derivative control gains\n",
    "    Kp = 0.1\n",
    "    Kd = 0.01\n",
    "\n",
    "    # PD control to compute actuator commands\n",
    "    position_error = desired_qpos - qpos\n",
    "    velocity_error = desired_qvel - qvel\n",
    "    control_signal = 0.01*(Kp * position_error + Kd * velocity_error)\n",
    "\n",
    "    # Clip the control signal to the action specification limits\n",
    "    action = np.clip(control_signal, action_spec.minimum, action_spec.maximum)\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Video of the movement{vertical-output: true}\n",
    "#@test {\"timeout\": 600}\n",
    "\n",
    "duration = 10   # (Seconds)\n",
    "framerate = 30  # (Hz)\n",
    "video = []\n",
    "\n",
    "# Control signal frequency, phase, amplitude.\n",
    "freq = 5\n",
    "amp = 0.9\n",
    "\n",
    "# Initialize a list to store rewards.\n",
    "rewards = []\n",
    "discount = []\n",
    "observation = []\n",
    "\n",
    "# Simulate, saving video frames and torso locations.\n",
    "env.physics.reset()\n",
    "while env.physics.data.time < duration:\n",
    "  # Inject controls and step the physics.\n",
    "  action = random_policy(time_step)\n",
    "  time_step = env.step(action)\n",
    "\n",
    "  # Store the reward\n",
    "  rewards.append(time_step.reward)\n",
    "  discount.append(time_step.discount)\n",
    "  observation.append(time_step.observation)\n",
    "  \n",
    "  # print(\"reward = {}, discount = {}, observations = {}.\".format(\n",
    "  #     time_step.reward, time_step.discount, time_step.observation))\n",
    "\n",
    "  # Save video frames.\n",
    "  if len(video) < env.physics.data.time * framerate:\n",
    "    pixels = env.physics.render(camera_id=1,**frame_size)\n",
    "    video.append(pixels.copy())\n",
    "\n",
    "display_video(video, framerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义图像特征提取网络\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=5, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # 计算最终的特征图大小以更新全连接层参数\n",
    "        # 64x64 -> conv1 (30x30) -> conv2 (13x13) -> conv3 (5x5)\n",
    "        self.fc = nn.Linear(64 * 5 * 5, 256)  # 假设输入图像经过卷积层后为7x7大小\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x  # 返回低维特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Gradient (PG)\n",
    "\n",
    "# 策略网络\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, obs_size, action_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 1024)\n",
    "        self.fc_mean = nn.Linear(1024, action_size)  # 输出动作的均值\n",
    "        self.fc_log_std = nn.Linear(1024, action_size)  # 输出动作的对数标准差\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        mean = self.fc_mean(x)\n",
    "        log_std = self.fc_log_std(x)\n",
    "        std = torch.exp(log_std)  # 转化为标准差\n",
    "        return mean, std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_network=CNNFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_tensor = torch.tensor(scene, dtype=torch.float32)\n",
    "# 添加 batch 和 channel 维度\n",
    "scene_tensor = scene_tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, 64, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = CNNFeatureExtractor()\n",
    "\n",
    "# # 将模型设置为评估模式（如果你不需要训练）\n",
    "# feature_extractor.eval()\n",
    "\n",
    "# 前向传递\n",
    "features = feature_extractor(scene_tensor)\n",
    "\n",
    "# 查看输出特征的形状\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the current positions and velocities\n",
    "qpos = env.physics.data.qpos[7:]\n",
    "qvel = env.physics.data.qvel[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_size = len(qpos)+len(qvel)+features.shape[1]\n",
    "action_size = len(qpos)\n",
    "\n",
    "# 初始化策略网络\n",
    "policy_network = PolicyNetwork(obs_size, action_size)\n",
    "\n",
    "# 定义联合模型参数优化\n",
    "params = list(feature_extractor.parameters()) + list(policy_network.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=1e-4)\n",
    "\n",
    "# 策略更新\n",
    "def update_policy(trajectory, gamma=0.99):\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for r in trajectory['rewards'][::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    \n",
    "    returns = torch.tensor(returns)\n",
    "    obs = torch.stack(trajectory['observations'])\n",
    "    # scene = torch.stack(trajectory['walkers/egocentric_camera'])\n",
    "    # feature = feature_extractor(scene)\n",
    "    \n",
    "    # print(\"actions size:\", trajectory['actions'])\n",
    "    actions = torch.stack(trajectory['actions'])\n",
    "    \n",
    "    means, stds = policy_network(obs)\n",
    "    action_distribution = torch.distributions.Normal(means, stds)\n",
    "    log_probs = action_distribution.log_prob(actions).sum(axis=-1)  # 对每个自由度计算对数概率并相加\n",
    "\n",
    "    loss = -(log_probs * returns).mean()  # 损失是负的期望回报\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss after each update\n",
    "    print(f\"Policy Update Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采样数据并更新\n",
    "trajectory = {'observations': [],'discount':[], 'actions': [], 'rewards': []}\n",
    "time_step = env.reset()\n",
    "qpos = time_step.observation['walker/joints_pos']\n",
    "qvel = time_step.observation['walker/joints_vel']\n",
    "\n",
    "scene = time_step.observation['walker/egocentric_camera']\n",
    "gray_scene = np.dot(scene[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "scene_tensor = torch.tensor(gray_scene, dtype=torch.float32)\n",
    "# 添加 batch 和 channel 维度\n",
    "scene_tensor = scene_tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, 64, 64]\n",
    "feature = feature_extractor(scene_tensor)\n",
    "feature = torch.squeeze(feature)\n",
    "\n",
    "obs = torch.cat([torch.tensor(qpos, dtype=torch.float32), torch.tensor(qvel, dtype=torch.float32), feature], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_size = {'width': 640, 'height': 480}\n",
    "\n",
    "duration = 20   # (Seconds)\n",
    "framerate = 30  # (Hz)\n",
    "video = []\n",
    "\n",
    "# Control signal frequency, phase, amplitude.\n",
    "freq = 5\n",
    "amp = 0.9\n",
    "\n",
    "while env.physics.data.time < duration:\n",
    "  # Inject controls and step the physics.\n",
    "  mean, std = policy_network(obs)  # 使用更新后的策略网络\n",
    "  std = torch.clamp(std, min=1e-3)  # 设置标准差的最小值为 1e-3\n",
    "  action_distribution = torch.distributions.Normal(mean, std)\n",
    "  action = action_distribution.sample()  # 采样生成下一步动作\n",
    "  action = torch.clamp(action, min=0.01, max=0.99)\n",
    "\n",
    "\n",
    "  time_step = env.step(action)\n",
    "\n",
    "  # Store the reward\n",
    "  trajectory['rewards'].append(time_step.reward)\n",
    "  trajectory['actions'].append(action)\n",
    "  trajectory['discount'].append(time_step.discount)\n",
    "  trajectory['observations'].append(time_step.observation)\n",
    "\n",
    "  qpos = time_step.observation['walker/joints_pos']\n",
    "  qvel = time_step.observation['walker/joints_vel']\n",
    "  scene = time_step.observation['walker/egocentric_camera']\n",
    "  gray_scene = np.dot(scene[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "  scene_tensor = torch.tensor(gray_scene, dtype=torch.float32)\n",
    "  # 添加 batch 和 channel 维度\n",
    "  scene_tensor = scene_tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, 64, 64]\n",
    "  feature = feature_extractor(scene_tensor)\n",
    "  feature = torch.squeeze(feature)\n",
    "  obs = torch.cat([torch.tensor(qpos, dtype=torch.float32), torch.tensor(qvel, dtype=torch.float32), feature], dim=0)\n",
    "  \n",
    "   # 检查是否为终止状态\n",
    "  if time_step.last():\n",
    "      update_policy(trajectory)  # 更新策略\n",
    "      trajectory = {'observations': [],'discount':[], 'actions': [], 'rewards': []}\n",
    "      time_step = env.reset()  # 重置环境\n",
    "      obs = torch.cat([torch.tensor(qpos, dtype=torch.float32), torch.tensor(qvel, dtype=torch.float32), feature], dim=0)\n",
    "\n",
    "  # print(\"reward = {}, discount = {}, observations = {}.\".format(\n",
    "  #     time_step.reward, time_step.discount, time_step.observation))\n",
    "\n",
    "  # Save video frames.\n",
    "  if len(video) < env.physics.data.time * framerate:\n",
    "    pixels = env.physics.render(camera_id=1,**frame_size)\n",
    "    video.append(pixels.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_video(video, framerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "max_episodes = 5\n",
    "\n",
    "# Initialize experience buffer\n",
    "buffer = {'observations': [], 'actions': [], 'rewards': [], 'next_observations': []}\n",
    "\n",
    "# Track rewards\n",
    "episode_rewards = []\n",
    "\n",
    "# Training loop for multiple episodes\n",
    "for episode in tqdm(range(max_episodes), desc=\"Training Episodes\"):\n",
    "    time_step = env.reset()\n",
    "    qpos = time_step.observation['walker/joints_pos']\n",
    "    qvel = time_step.observation['walker/joints_vel']\n",
    "    scene = time_step.observation['walker/egocentric_camera']\n",
    "    gray_scene = np.dot(scene[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "    scene_tensor = torch.tensor(gray_scene, dtype=torch.float32)\n",
    "    # 添加 batch 和 channel 维度\n",
    "    scene_tensor = scene_tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, 64, 64]\n",
    "    feature = feature_extractor(scene_tensor)\n",
    "    feature = torch.squeeze(feature)\n",
    "    obs = torch.cat([torch.tensor(qpos, dtype=torch.float32), torch.tensor(qvel, dtype=torch.float32), feature], dim=0)\n",
    "\n",
    "    episode_reward = 0  # Reset episode reward\n",
    "    step = 0\n",
    "    with tqdm(desc=f\"Episode {episode+1}\", leave=False) as pbar:\n",
    "        while not time_step.last():\n",
    "            # Sample action from policy\n",
    "            mean, std = policy_network(obs)\n",
    "            std = torch.clamp(std, min=1e-3)\n",
    "            action_distribution = torch.distributions.Normal(mean, std)\n",
    "            action = action_distribution.sample()\n",
    "            action = torch.clamp(action, min=0.01, max=0.99)\n",
    "\n",
    "            # Step the environment\n",
    "            next_time_step = env.step(action)\n",
    "\n",
    "            # Store in buffer\n",
    "            buffer['observations'].append(obs)\n",
    "            buffer['actions'].append(action)\n",
    "            buffer['rewards'].append(next_time_step.reward)\n",
    "\n",
    "            # Update the cumulative reward for the episode\n",
    "            episode_reward += next_time_step.reward\n",
    "\n",
    "            # Prepare next observation\n",
    "            qpos_next = next_time_step.observation['walker/joints_pos']\n",
    "            qvel_next = next_time_step.observation['walker/joints_vel']\n",
    "\n",
    "            scene_next = next_time_step.observation['walker/egocentric_camera']\n",
    "            gray_scene = np.dot(scene[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "            scene_tensor = torch.tensor(gray_scene, dtype=torch.float32)\n",
    "            # 添加 batch 和 channel 维度\n",
    "            scene_tensor = scene_tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, 64, 64]\n",
    "            feature = feature_extractor(scene_tensor)\n",
    "            feature_next = torch.squeeze(feature)\n",
    "            next_obs = torch.cat([torch.tensor(qpos, dtype=torch.float32), torch.tensor(qvel, dtype=torch.float32), feature_next], dim=0)\n",
    "\n",
    "            buffer['next_observations'].append(next_obs)\n",
    "\n",
    "            obs = next_obs\n",
    "            time_step = next_time_step\n",
    "            step += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "     # Store episode reward\n",
    "    episode_rewards.append(episode_reward)\n",
    "\n",
    "    # After episode ends, update policy using the buffer\n",
    "    update_policy(buffer)\n",
    "\n",
    "    # Reset buffer for next episode\n",
    "    buffer = {'observations': [], 'actions': [], 'rewards': [], 'next_observations': []}\n",
    "\n",
    "\n",
    "    # Print episode reward\n",
    "    print(f\"Episode {episode + 1}/{max_episodes}, Reward: {episode_reward}\")\n",
    "    print(f\"step_num: {step}\")\n",
    "\n",
    "# After all episodes, print the reward list\n",
    "print(\"All Episode Rewards:\", episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot episode rewards after training\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Rewards over Episodes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy_network, num_episodes=5):\n",
    "    total_eval_reward = 0\n",
    "    for eval_episode in range(num_episodes):\n",
    "        time_step = env.reset()\n",
    "        qpos = time_step.observation['walker/joints_pos']\n",
    "        qvel = time_step.observation['walker/joints_vel']\n",
    "        obs = torch.cat([torch.tensor(qpos, dtype=torch.float32), torch.tensor(qvel, dtype=torch.float32)], dim=0)\n",
    "\n",
    "        episode_reward = 0\n",
    "        while not time_step.last():\n",
    "            with torch.no_grad():  # Don't update the policy during evaluation\n",
    "                mean, std = policy_network(obs)\n",
    "                action_distribution = torch.distributions.Normal(mean, std)\n",
    "                action = action_distribution.sample()\n",
    "                action = torch.clamp(action, min=0.01, max=0.99)\n",
    "\n",
    "            # Step the environment\n",
    "            time_step = env.step(action)\n",
    "            episode_reward += time_step.reward\n",
    "\n",
    "            # Prepare next observation\n",
    "            qpos_next = time_step.observation['walker/joints_pos']\n",
    "            qvel_next = time_step.observation['walker/joints_vel']\n",
    "            obs = torch.cat([torch.tensor(qpos_next, dtype=torch.float32), torch.tensor(qvel_next, dtype=torch.float32)], dim=0)\n",
    "\n",
    "        total_eval_reward += episode_reward\n",
    "        print(f\"Evaluation Episode {eval_episode + 1}/{num_episodes}, Reward: {episode_reward}\")\n",
    "\n",
    "    avg_eval_reward = total_eval_reward / num_episodes\n",
    "    print(f\"Average Evaluation Reward over {num_episodes} episodes: {avg_eval_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor-Critic (AC)\n",
    "\n",
    "# 值网络\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = torch.relu(self.fc1(obs))\n",
    "        return self.fc2(x)  # 输出状态的价值\n",
    "\n",
    "# 初始化值网络\n",
    "value_network = ValueNetwork(obs_dim)\n",
    "value_optimizer = optim.Adam(value_network.parameters(), lr=1e-3)\n",
    "\n",
    "# 更新步骤\n",
    "def update_actor_critic(trajectory, gamma=0.99):\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for r in trajectory['rewards'][::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "\n",
    "    returns = torch.tensor(returns)\n",
    "    obs = torch.stack(trajectory['observations'])\n",
    "    actions = torch.tensor(trajectory['actions'])\n",
    "    \n",
    "    values = value_network(obs).squeeze()\n",
    "    advantages = returns - values  # 计算优势函数\n",
    "    \n",
    "    # 更新 Actor\n",
    "    log_probs = torch.log(policy_network(obs).gather(1, actions.unsqueeze(1)))\n",
    "    policy_loss = -(log_probs * advantages.detach()).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 更新 Critic\n",
    "    value_loss = nn.functional.mse_loss(values, returns)\n",
    "    \n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    value_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
