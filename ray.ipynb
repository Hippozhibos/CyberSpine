{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "config = (  # 1. Configure the algorithm,\n",
    "    PPOConfig()\n",
    "    .environment(\"Taxi-v3\")\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .framework(\"torch\")\n",
    "    .training(model={\"fcnet_hiddens\": [64, 64]})\n",
    "    .evaluation(evaluation_num_env_runners=1)\n",
    ")\n",
    "\n",
    "algo = config.build()  # 2. build the algorithm,\n",
    "\n",
    "for _ in range(5):\n",
    "    print(algo.train())  # 3. train it,\n",
    "\n",
    "algo.evaluate()  # 4. and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=True,\n",
    "        enable_env_runner_and_connector_v2=True,\n",
    "    )\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .env_runners(num_env_runners=1)\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "\n",
    "for i in range(10):\n",
    "    result = algo.train()\n",
    "    result.pop(\"config\")\n",
    "    pprint(result)\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo.save()\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import train, tune\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=True,\n",
    "        enable_env_runner_and_connector_v2=True,\n",
    "    )\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .training(\n",
    "        lr=tune.grid_search([0.01, 0.001, 0.0001]),\n",
    "    )\n",
    ")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=config,\n",
    "    run_config=train.RunConfig(\n",
    "        stop={\"env_runners/episode_return_mean\": 150.0},\n",
    "    ),\n",
    ")\n",
    "\n",
    "tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import train, tune\n",
    "\n",
    "# Tuner.fit() allows setting a custom log directory (other than ~/ray-results).\n",
    "tuner = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=config,\n",
    "    run_config=train.RunConfig(\n",
    "        stop={\"num_env_steps_sampled_lifetime\": 20000},\n",
    "        checkpoint_config=train.CheckpointConfig(checkpoint_at_end=True),\n",
    "    ),\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "\n",
    "# Get the best result based on a particular metric.\n",
    "best_result = results.get_best_result(\n",
    "    metric=\"env_runners/episode_return_mean\", mode=\"max\"\n",
    ")\n",
    "\n",
    "# Get the best checkpoint corresponding to the best result.\n",
    "best_checkpoint = best_result.checkpoint\n",
    "\n",
    "print(f\"Best learning rate: {best_result.config['lr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from ray.rllib.core.rl_module import RLModule\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Create only the neural network (RLModule) from our checkpoint.\n",
    "rl_module = RLModule.from_checkpoint(\n",
    "    pathlib.Path(best_checkpoint.path) / \"learner_group\" / \"learner\" / \"rl_module\"\n",
    ")[\"default_policy\"]\n",
    "\n",
    "episode_return = 0\n",
    "terminated = truncated = False\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "while not terminated and not truncated:\n",
    "    # Compute the next action from a batch (B=1) of observations.\n",
    "    torch_obs_batch = torch.from_numpy(np.array([obs]))\n",
    "    action_logits = rl_module.forward_inference({\"obs\": torch_obs_batch})[\n",
    "        \"action_dist_inputs\"\n",
    "    ]\n",
    "    # The default RLModule used here produces action logits (from which\n",
    "    # we'll have to sample an action or use the max-likelihood one).\n",
    "    action = torch.argmax(action_logits[0]).numpy()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    episode_return += reward\n",
    "\n",
    "print(f\"Reached episode return of {episode_return}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run to install MuJoCo and `dm_control`\n",
    "import distutils.util\n",
    "import os\n",
    "import subprocess\n",
    "if subprocess.run('nvidia-smi').returncode:\n",
    "  raise RuntimeError(\n",
    "      'Cannot communicate with GPU. '\n",
    "      'Make sure you are using a GPU Colab runtime. '\n",
    "      'Go to the Runtime menu and select Choose runtime type.')\n",
    "\n",
    "# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
    "# This is usually installed as part of an Nvidia driver package, but the Colab\n",
    "# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
    "# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
    "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
    "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
    "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
    "    f.write(\"\"\"{\n",
    "    \"file_format_version\" : \"1.0.0\",\n",
    "    \"ICD\" : {\n",
    "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# print('Installing dm_control...')\n",
    "# !pip install -q dm_control>=1.0.18\n",
    "\n",
    "# Configure dm_control to use the EGL rendering backend (requires GPU)\n",
    "%env MUJOCO_GL=egl\n",
    "\n",
    "print('Checking that the dm_control installation succeeded...')\n",
    "try:\n",
    "  from dm_control import suite\n",
    "  env = suite.load('cartpole', 'swingup')\n",
    "  pixels = env.physics.render()\n",
    "except Exception as e:\n",
    "  raise e from RuntimeError(\n",
    "      'Something went wrong during installation. Check the shell output above '\n",
    "      'for more information.\\n'\n",
    "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
    "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
    "else:\n",
    "  del pixels, suite\n",
    "\n",
    "!echo Installed dm_control $(pip show dm_control | grep -Po \"(?<=Version: ).+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Other imports and helper functions\n",
    "\n",
    "# General\n",
    "import copy\n",
    "import os\n",
    "import itertools\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "# Graphics-related\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import PIL.Image\n",
    "# Internal loading of video libraries.\n",
    "\n",
    "# Use svg backend for figure rendering\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# Font sizes\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 10\n",
    "BIGGER_SIZE = 12\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "# Inline video helper function\n",
    "if os.environ.get('COLAB_NOTEBOOK_TEST', False):\n",
    "  # We skip video generation during tests, as it is quite expensive.\n",
    "  display_video = lambda *args, **kwargs: None\n",
    "else:\n",
    "  def display_video(frames, framerate=30):\n",
    "    height, width, _ = frames[0].shape\n",
    "    dpi = 70\n",
    "\n",
    "    orig_backend = matplotlib.get_backend()\n",
    "    matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "    # fig, ax = plt.subplots(1, 1, figsize=(scaled_width / dpi, scaled_height / dpi), dpi=dpi)\n",
    "    matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
    "    ax.set_axis_off()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_position([0, 0, 1, 1])\n",
    "    im = ax.imshow(frames[0])\n",
    "    def update(frame):\n",
    "      im.set_data(frame)\n",
    "      return [im]\n",
    "    interval = 1000/framerate\n",
    "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                   interval=interval, blit=True, repeat=False)\n",
    "    return HTML(anim.to_html5_video())\n",
    "\n",
    "# Seed numpy's global RNG so that cell outputs are deterministic. We also try to\n",
    "# use RandomState instances that are local to a single cell wherever possible.\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the \"env\" folder to Python's module search path\n",
    "env_path = \"/home/zhangzhibo/CyberSpine\"\n",
    "if env_path not in sys.path:\n",
    "    sys.path.append(env_path)\n",
    "\n",
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(runtime_env={\"py_modules\": [\"/home/zhangzhibo/CyberSpine\"]})\n",
    "\n",
    "@ray.remote\n",
    "def check_path():\n",
    "    import sys\n",
    "    return sys.path\n",
    "\n",
    "print(ray.get(check_path.remote()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import mice_env\n",
    "import shimmy\n",
    "from dm_control import suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_mice_env = shimmy.DmControlCompatibilityV0(mice_env.rodent_maze_forage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "# 注册自定义环境\n",
    "register(\n",
    "    id='MiceEnv-v0',\n",
    "    entry_point=lambda: shimmy.DmControlCompatibilityV0(mice_env.rodent_maze_forage())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from shimmy.dm_control_compatibility import DmControlCompatibilityV0\n",
    "from env import mice_env\n",
    "\n",
    "def create_custom_env(cfg):\n",
    "    return DmControlCompatibilityV0(mice_env.rodent_maze_forage())\n",
    "\n",
    "# Register the environment with Ray\n",
    "tune.register_env(\"MiceEnv-v0\", lambda cfg: create_custom_env(cfg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# 使用注册的环境ID\n",
    "config = PPOConfig().environment(\"MiceEnv-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = config.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Record\n",
    "\n",
    "24/10/15 Debug:\n",
    "1. parser.parse_args() 的问题在于 Jupyter Notebook 本身不通过命令行运行，因此无法传递参数给 argparse。\n",
    "2. 如果你想在 .ipynb 文件中传递参数，可以手动设置参数，而不是从命令行解析。\n",
    "\n",
    "24/10/16 Debug:\n",
    "1. RLlib 不接受用function定义的环境作为.environment的输入，\n",
    "2. 必须将maze_forazing()改为gym环境或者自定义为class变量输入给env\n",
    "3. 重点考虑手动定义class的方法\n",
    "\n",
    "24/10/18 Debug:\n",
    "1. 仔细看RLlib 定义custom env的方法 https://docs.ray.io/en/latest/rllib/rllib-env.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from ray.rllib.algorithms.dreamerv3.dreamerv3 import DreamerV3Config\n",
    "from ray.rllib.utils.test_utils import add_rllib_example_script_args\n",
    "\n",
    "sys.argv = ['script_name','--num-gpus','1','--num-env-runners','4']\n",
    "\n",
    "parser = add_rllib_example_script_args(\n",
    "    default_iters=1000000,\n",
    "    default_reward=800.0,\n",
    "    default_timesteps=1000000\n",
    ")\n",
    "# Use `parser` to add your own custom command line options to this script\n",
    "# and (if needed) use their values toset up `config` below.\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "config = (\n",
    "    DreamerV3Config()\n",
    "    # Use image observations.\n",
    "    .environment(\n",
    "        env='MiceEnv-v0',\n",
    "        env_config={\"from_pixels\": True},\n",
    "    )\n",
    "    .learners(\n",
    "        num_learners=0 if args.num_gpus == 1 else args.num_gpus,\n",
    "        num_gpus_per_learner=1 if args.num_gpus else 0,\n",
    "    )\n",
    "    .env_runners(\n",
    "        num_env_runners=(args.num_env_runners or 0),\n",
    "        # If we use >1 GPU and increase the batch size accordingly, we should also\n",
    "        # increase the number of envs per worker.\n",
    "        num_envs_per_env_runner=4 * (args.num_gpus or 1),\n",
    "        remote_worker_envs=True,\n",
    "    )\n",
    "    .reporting(\n",
    "        metrics_num_episodes_for_smoothing=(args.num_gpus or 1),\n",
    "        report_images_and_videos=False,\n",
    "        report_dream_data=False,\n",
    "        report_individual_batch_item_stats=False,\n",
    "    )\n",
    "    # See Appendix A.\n",
    "    .training(\n",
    "        model_size=\"S\",\n",
    "        training_ratio=512,\n",
    "        batch_size_B=16 * (args.num_gpus or 1),\n",
    "    )\n",
    "\n",
    ")\n",
    "\n",
    "config.remote_worker_envs=False\n",
    "rllib_algo = config.build(use_copy=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
